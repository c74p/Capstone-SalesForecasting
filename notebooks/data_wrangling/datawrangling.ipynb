{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got  google googletrend.csv\n",
      "Got  state_names state_names.csv\n",
      "Got  stores store.csv\n",
      "Got  store_states store_states.csv\n",
      "Got  train train.csv\n",
      "Got  weather weather.csv\n"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "names = ['google', 'state_names', 'stores', 'store_states', 'train', 'weather']  \n",
    "\n",
    "files = ['googletrend.csv', 'state_names.csv', 'store.csv', 'store_states.csv', 'train.csv', 'weather.csv']  \n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for name, file in zip(names, files):\n",
    "    dfs[name] =  pd.read_csv(f'../../data/raw/{file}', low_memory=False)\n",
    "    print(\"Got \", name, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = dfs['google'].copy()\n",
    "state_names = dfs['state_names'].copy()  # Done\n",
    "stores = dfs['stores'].copy()\n",
    "store_states = dfs['store_states'].copy()  # Done\n",
    "train = dfs['train'].copy()  # Done\n",
    "weather = dfs['weather'].copy()\n",
    "all_dfs = [google, state_names, stores, store_states, train, weather]\n",
    "\n",
    "# Create _raw copies for reference\n",
    "google_raw = dfs['google']\n",
    "state_names_raw = dfs['state_names']\n",
    "stores_raw = dfs['stores']\n",
    "store_states_raw = dfs['store_states']\n",
    "train_raw = dfs['train']\n",
    "weather_raw = dfs['weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spelling error in weather dataframe\n",
    "if 'Min_VisibilitykM' in weather.columns:\n",
    "    weather.rename(columns = {'Min_VisibilitykM':'Min_VisibilityKm'}, inplace=True)\n",
    "\n",
    "def convert_to_snake_case(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    draft = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    return draft.replace('__', '_')\n",
    "\n",
    "for df in all_dfs:\n",
    "    col_list = list(df.columns)\n",
    "    df.columns = pd.Index(map(convert_to_snake_case, col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store State\n",
       "0      1    HE\n",
       "1      2    TH\n",
       "2      3    NW\n",
       "3      4    BE\n",
       "4      5    SN"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_states_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info()   - Stores - CompetitionOpenSinceMonth fill with mean                                      \n",
    "#   - Stores - CompetitionOpenSinceYear fill with mean                                       \n",
    "#   - Stores - Promo2SinceWeek fill with zero                                                \n",
    "#   - Stores - Promo2SinceYear fill with zero                                                \n",
    "#   - Stores - PromoInterval fill with zero                                                  \n",
    "#   - Weather - Max_VisibityKm fill with mean                                                \n",
    "#   - Weather - Min_VisibitykM fill with mean                                                \n",
    "#   - Weather - Mean_VisibityKm fill with mean                                               \n",
    "#   - Weather - Max_Gust_SpeedKm_h fill with mean                                            \n",
    "#   - Weather - CloudCover fill with mean                                                    \n",
    "#   - Weather - CloudCover fill with string 'No events'    \n",
    "\n",
    "stores['promo2_since_week'] = stores.promo2_since_week.fillna(stores.promo2_since_week.mean())\n",
    "stores['promo2_since_year'] = stores.promo2_since_year.fillna(stores.promo2_since_year.mean())\n",
    "stores['promo_interval'] = stores.promo_interval.fillna('None')\n",
    "stores['competition_distance'] = stores.competition_distance.fillna(stores.competition_distance.mean())\n",
    "stores['competition_open_since_month'] = stores.competition_open_since_month.fillna(stores.competition_open_since_month.mean())\n",
    "stores['competition_open_since_year'] = stores.competition_open_since_year.fillna(stores.competition_open_since_year.mean())\n",
    "\n",
    "weather['max_visibility_km'] = weather.max_visibility_km.fillna(weather.max_visibility_km.mean())\n",
    "weather['min_visibility_km'] = weather.min_visibility_km.fillna(weather.min_visibility_km.mean())\n",
    "weather['mean_visibility_km'] = weather.mean_visibility_km.fillna(weather.mean_visibility_km.mean())\n",
    "weather['max_gust_speed_km_h'] = weather.max_gust_speed_km_h.fillna(weather.max_gust_speed_km_h.mean())\n",
    "weather['cloud_cover'] = weather.cloud_cover.fillna(weather.cloud_cover.mean())\n",
    "weather['events'] = weather.events.fillna('No Events')\n",
    "\n",
    "# ADD OTHER CHECKS LIKE THIS\n",
    "# stores_raw.Promo2SinceWeek.mean() == stores.Promo2SinceWeek.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep\n",
    "google['state'] = google.file.str[-2:]\n",
    "google.loc[google.state == 'NI', 'state'] = 'HB,NI'\n",
    "google['week_start'] = pd.to_datetime(google.week.str[:10])\n",
    "\n",
    "# Add date to google dataframe, aligned with starting week\n",
    "start_date = pd.to_datetime(google.week.min()[:10])\n",
    "end_date = pd.to_datetime(google.week.max()[-10:])\n",
    "days = np.arange(start_date, end_date + pd.to_timedelta('1D'), pd.to_timedelta('1D'))\n",
    "weeks = np.arange(start_date, end_date + pd.to_timedelta('1D'), pd.to_timedelta('7D'))\n",
    "all_weeks = pd.Series(np.hstack([weeks for i in range(0,7)])) # 1036\n",
    "week_lookup = pd.DataFrame({'date': days, 'Week_Start': all_weeks})\n",
    "google = week_lookup.merge(google, left_on='Week_Start', right_on='week_start')\n",
    "google = google.drop(['file', 'Week_Start', 'week'], axis='columns') #len=14504\n",
    "\n",
    "# Merge order after changing files:\n",
    "# store_states and state_names, on='State'\n",
    "# that and weather, left_on='StateName', right_on='file'\n",
    "# that and google, on=['Date', 'State'], ??how='outer'??\n",
    "# that and stores, on='Store'\n",
    "# that and train, on=['Date', 'Store'], ??how='outer'??\n",
    "# all_dfs = [google, state_names, stores, store_states, train, weather]\n",
    "# [, train]\n",
    "\n",
    "# Keep and refactor\n",
    "df = store_states.merge(state_names, on='state')\n",
    "df = df.merge(weather, left_on='state_name', right_on='file').drop('file', axis='columns')\n",
    "remove_excess_dates = (df.date >= '2013-01-01') & (df.date <= '2015-07-31')\n",
    "df = df[remove_excess_dates]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.merge(google, on=['date', 'state'])\n",
    "df = df.merge(stores, on='store')\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "df = df.merge(train, on=['date', 'store'], how='outer')\n",
    "df.loc[df.open.isnull(), 'open'] = 0\n",
    "df.loc[df.sales.isnull(), 'sales'] = 0\n",
    "df.loc[df.customers.isnull(), 'customers'] = 0\n",
    "df.loc[df.promo.isnull(), 'promo'] = 0\n",
    "df.loc[df.school_holiday.isnull(), 'school_holiday'] = 0\n",
    "df.loc[df.state_holiday.isnull(), 'state_holiday'] = 0\n",
    "df['day_of_week'] = df.date.dt.dayofweek\n",
    "# do we need week_start later?\n",
    "#df.isnull().sum()\n",
    "# day_of_week, sales, customers, open, promo, state_holiday, school_holiday\n",
    "# state_holiday, school_holiday\n",
    "\n",
    "# do we need how=,outer' for google? Or come back and revisit to see if lagging trend is useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 2 columns):\n",
      "StateName    16 non-null object\n",
      "State        16 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 336.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#names = ['google', 'state_names', 'stores', 'store_states', 'train', 'weather']  \n",
    "\n",
    "state_names_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1050330 entries, 0 to 1050329\n",
      "Data columns (total 44 columns):\n",
      "store                           1050330 non-null int64\n",
      "state                           1050330 non-null object\n",
      "state_name                      1050330 non-null object\n",
      "date                            1050330 non-null datetime64[ns]\n",
      "max_temperature_c               1050330 non-null int64\n",
      "mean_temperature_c              1050330 non-null int64\n",
      "min_temperature_c               1050330 non-null int64\n",
      "dew_point_c                     1050330 non-null int64\n",
      "mean_dew_point_c                1050330 non-null int64\n",
      "min_dewpoint_c                  1050330 non-null int64\n",
      "max_humidity                    1050330 non-null int64\n",
      "mean_humidity                   1050330 non-null int64\n",
      "min_humidity                    1050330 non-null int64\n",
      "max_sea_level_pressureh_pa      1050330 non-null int64\n",
      "mean_sea_level_pressureh_pa     1050330 non-null int64\n",
      "min_sea_level_pressureh_pa      1050330 non-null int64\n",
      "max_visibility_km               1050330 non-null float64\n",
      "mean_visibility_km              1050330 non-null float64\n",
      "min_visibility_km               1050330 non-null float64\n",
      "max_wind_speed_km_h             1050330 non-null int64\n",
      "mean_wind_speed_km_h            1050330 non-null int64\n",
      "max_gust_speed_km_h             1050330 non-null float64\n",
      "precipitationmm                 1050330 non-null float64\n",
      "cloud_cover                     1050330 non-null float64\n",
      "events                          1050330 non-null object\n",
      "wind_dir_degrees                1050330 non-null int64\n",
      "trend                           1050330 non-null int64\n",
      "week_start                      1050330 non-null datetime64[ns]\n",
      "store_type                      1050330 non-null object\n",
      "assortment                      1050330 non-null object\n",
      "competition_distance            1050330 non-null float64\n",
      "competition_open_since_month    1050330 non-null float64\n",
      "competition_open_since_year     1050330 non-null float64\n",
      "promo2                          1050330 non-null int64\n",
      "promo2_since_week               1050330 non-null float64\n",
      "promo2_since_year               1050330 non-null float64\n",
      "promo_interval                  1050330 non-null object\n",
      "day_of_week                     1050330 non-null int64\n",
      "sales                           1050330 non-null float64\n",
      "customers                       1050330 non-null float64\n",
      "open                            1050330 non-null float64\n",
      "promo                           1050330 non-null float64\n",
      "state_holiday                   1050330 non-null object\n",
      "school_holiday                  1050330 non-null float64\n",
      "dtypes: datetime64[ns](2), float64(16), int64(19), object(7)\n",
      "memory usage: 360.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.isnull().any()\n",
    "df.info()\n",
    "#len(df)    # 1050330\n",
    "#942 * 1115 # 1050330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.70493273542601"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train) # 1017209\n",
    "# train.Date.nunique() # 942\n",
    "# train.Store.nunique() # 1115\n",
    "1115 * 942 # 1050330 \n",
    "1050330 - 1017209 # 33121\n",
    "33121/942 # 35.16...\n",
    "33121/1115 # 29.70...\n",
    "# train.groupby('Store')['Date'].count().value_counts()\n",
    "#    942    934\n",
    "#    758    180\n",
    "#    941      1\n",
    "# 934 * 942 + 180 * 758 + 941 # 1017209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'the label [date] is not in the [columns]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m                     \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1784\u001b[0m                                .format(key=key,\n\u001b[0;32m-> 1785\u001b[0;31m                                        axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the label [date] is not in the [columns]'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-693-55a59a34e9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpromo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#629129\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpromo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#388080\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschool_holiday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 185\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_holiday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 185\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_holiday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0;31m# we have yielded a scalar ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1796\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m                 \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Capstone2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 raise KeyError(u\"the label [{key}] is not in the [{axis}]\"\n\u001b[1;32m   1784\u001b[0m                                .format(key=key,\n\u001b[0;32m-> 1785\u001b[0;31m                                        axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the label [date] is not in the [columns]'"
     ]
    }
   ],
   "source": [
    "train.promo.unique()\n",
    "len(train[train.promo == 0]) #629129\n",
    "len(train[train.promo == 1]) #388080\n",
    "len(df.loc[df.school_holiday.isnull(), 'date'].unique()) # 185\n",
    "len(df.loc[df.state_holiday.isnull(), 'date'].unique()) # 185\n",
    "dates = np.sort(df.loc[df.state_holiday.isnull(), 'date'].unique())\n",
    "#state_names_raw.columns\n",
    "#df.state_holiday.unique()\n",
    "#df.loc[df.state_holiday.isnull(), ['date', 'state', 'state_holiday']].head()\n",
    "dates[:15]\n",
    "#type(dates)\n",
    "#df.loc[df.date.apply(lambda x: x in dates), ['date', 'state', 'state_holiday']].head()\n",
    "df.loc[df.date == '2014-07-04T00:00:00.000000000', ['date', 'state', 'state_holiday']].head()\n",
    "len(df[df.state_holiday.isnull()])\n",
    "#df['date_state'] = df.date.dt.strftime('%Y-%m-%d') + df.state\n",
    "df.date_state.tail()\n",
    "df.loc[df.state_holiday.isnull(), 'date_state'].nunique() # 185\n",
    "date_states = np.sort(df.loc[df.state_holiday.isnull(), 'date_state'].unique())\n",
    "#df[df.date_state]\n",
    "date_states[:15]\n",
    "#df.loc[(df.date=='2014-07-03') & (df.state == 'BY') & (df.state_holiday.notnull()), 'state_holiday']\n",
    "df.head()\n",
    "df = df.set_index('date') #44\n",
    "date_mask = dates\n",
    "df[date_mask].head()\n",
    "#df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "# df.isnull().sum()\n",
    "sorted(df.day_of_week.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.info()\n",
    "#df.info()\n",
    "train.head()\n",
    "len(train) # 1017209\n",
    "len(df) # 1050330\n",
    "#train['date'] = pd.to_datetime(train['date'])\n",
    "#df = df.merge(train, on=['date', 'store'])\n",
    "len(df) # 1017209\n",
    "train.date.nunique() # 942\n",
    "train.store.nunique() # 1115\n",
    "1115 * 942 # 1050330\n",
    "df.date.nunique() # 942\n",
    "df.store.nunique() # 1115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester = train.merge(stores.merge(store_states.merge(state_names, on='State'), on='Store'), on='Store')\n",
    "# Still left: google, weather\n",
    "# len(train.columns) # 9\n",
    "#tester = train.merge(tester)\n",
    "#len(train.Store)\n",
    "#train.Store.nunique()\n",
    "# tester\n",
    "len(sorted(weather.file.unique())) # 16\n",
    "#len(sorted(tester.StateName.unique())) #12\n",
    "# tester.head()\n",
    "#print(sorted(tester.StateName.unique())) #12 missing Brandenburg, Bremen, MecklenburgVorpommern, Saarland\n",
    "(sorted(weather.file.unique())) # 16\n",
    "#sorted(weather.columns)#()\n",
    "# len(tester.State.unique()) #12\n",
    "#len(tester.StateName.unique()) #12\n",
    "\n",
    "\n",
    "# This is the command that led to a runaway process\n",
    "# tester.merge(weather, left_on='StateName', right_on='file').head()\n",
    "\n",
    "# stores2, store_states, state_names, weather2 - only left google and train\n",
    "# tester = (google.merge(stores2.merge(store_states.merge(state_names, on='State'), on='Store')).merge(weather2, left_on='StateName', right_on='file')\n",
    "tester.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(google.head())\n",
    "len(google.file.unique()) # 14\n",
    "len(tester.State.unique()) # 12\n",
    "len(tester.StateName.unique()) # 12\n",
    "len(tester.file.unique()) # 12\n",
    "sorted(weather.file.unique())\n",
    "# ['BadenWuerttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hessen', 'MecklenburgVorpommern', 'Niedersachsen', 'NordrheinWestfalen', 'RheinlandPfalz', 'Saarland', 'Sachsen', 'SachsenAnhalt', 'SchleswigHolstein', 'Thueringen']\n",
    "sorted(google.file.unique())\n",
    "# ['Rossmann_DE', 'Rossmann_DE_BE', 'Rossmann_DE_BW', 'Rossmann_DE_BY', 'Rossmann_DE_HE', 'Rossmann_DE_HH', 'Rossmann_DE_NI', 'Rossmann_DE_NW', 'Rossmann_DE_RP', 'Rossmann_DE_SH', 'Rossmann_DE_SL', 'Rossmann_DE_SN', 'Rossmann_DE_ST', 'Rossmann_DE_TH']\n",
    "state_names.head()\n",
    "len(state_names.State.unique()) # 16\n",
    "(state_names.State.unique()) # 16\n",
    "# 'BW', 'BY', 'BE', 'BB', 'HB', 'HH', 'HE', 'MV', 'HB,NI', 'NW', 'RP', 'SL', 'SN', 'ST', 'SH', 'TH']\n",
    "# len(state_names.StateName.unique()) # 16\n",
    "(state_names.StateName.unique()) # 16\n",
    "# ['BadenWuerttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hessen', 'MecklenburgVorpommern', 'Niedersachsen', 'NordrheinWestfalen', 'RheinlandPfalz', 'Saarland', 'Sachsen', 'SachsenAnhalt', 'SchleswigHolstein', 'Thueringen'],\n",
    "state_names\n",
    "google2 = google.copy()\n",
    "google2['State'] = google2.file.str[-2:]\n",
    "len(google2.State.unique()) # 14\n",
    "len(google.file.unique()) #14\n",
    "\n",
    "sorted(google2.State.unique()) # 14\n",
    "# ['BE', 'BW', 'BY', 'DE', 'HE', 'HH', 'NI', 'NW', 'RP', 'SH', 'SL', 'SN', 'ST', 'TH']\n",
    "#sorted(google.file.unique()) # 14\n",
    "#['Rossmann_DE',\n",
    "# 'Rossmann_DE_BE',\n",
    "# 'Rossmann_DE_BW',\n",
    "# 'Rossmann_DE_BY',\n",
    "# 'Rossmann_DE_HE',\n",
    "# 'Rossmann_DE_HH',\n",
    "# 'Rossmann_DE_NI',\n",
    "# 'Rossmann_DE_NW',\n",
    "# 'Rossmann_DE_RP',\n",
    "# 'Rossmann_DE_SH',\n",
    "# 'Rossmann_DE_SL',\n",
    "# 'Rossmann_DE_SN',\n",
    "# 'Rossmann_DE_ST',\n",
    "# 'Rossmann_DE_TH']\n",
    "sorted(state_names.StateName.unique()) # 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(stores.Store.values) == list(range(1,1116)) # True\n",
    "# stores.CompetitionOpenSinceMonth.hist()\n",
    "#stores[stores.CompetitionOpenSinceYear>2000].CompetitionOpenSinceYear.hist()\n",
    "stores.CompetitionOpenSinceYear.mean() # 2008\n",
    "#stores[stores.CompetitionOpenSinceYear>1980].CompetitionOpenSinceYear.mean() # 2008\n",
    "stores[stores.CompetitionOpenSinceYear>2000].CompetitionOpenSinceYear.mean() # 2009\n",
    "stores.CompetitionOpenSinceMonth.mean() # 7\n",
    "stores.describe()\n",
    "#stores.Promo2.unique() # Binary, roughly half of stores have Promo2\n",
    "#train.Store.nunique() #1115\n",
    "len(stores[stores.Promo2 == 0]) #544\n",
    "len(stores[stores.Promo2SinceWeek.isnull()]) #544\n",
    "len(stores[stores.Promo2SinceYear.isnull()]) #544\n",
    "stores[stores.Promo2SinceWeek.notnull()].head(15)\n",
    "stores.PromoInterval.unique() # Either starts in Jan, starts in Feb, or starts in Mar, with quarterly restarts\n",
    "#stores.info()\n",
    "#stores.Promo2SinceWeek.hist()\n",
    "#weather.info() # look out for Max_vis, Mean_vis, Min_vis, max_gust, cloudcover, events\n",
    "weather.Events.describe()\n",
    "#weather.Max_VisibilityKm.mean() # 24.0576\n",
    "#weather.Min_VisibilitykM.mean()  # 7.0252\n",
    "#weather.Mean_VisibilityKm.mean() # 12.2398\n",
    "#24.0576-7.0252 # 17.032\n",
    "#(weather.Max_VisibilityKm >= weather.Min_VisibilitykM).all()\n",
    "#weather[(weather.Max_VisibilityKm.notnull()) & (weather.Min_VisibilitykM.notnull()) & (weather.Max_VisibilityKm < weather.Min_VisibilitykM)] # 15,459\n",
    "#len(weather) # 15,840\n",
    "weather.Events.value_counts()\n",
    "len(weather[weather.Events.isnull()]) #3591\n",
    "len(weather) #15840\n",
    "weather.Events.describe() #11889\n",
    "11889+3591\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google.info()\n",
    "google.describe()\n",
    "# google.week.min() # 2012-12-02 - 2012-12-08\n",
    "# google.week.max() # 2015-09-27 - 2015-10-03\n",
    "# google.file.nunique() # 14\n",
    "# google.week.nunique() # 148 = total file size / 14\n",
    "sorted(google.file.unique())\n",
    "\n",
    "# train.Date.min()  # 2013-01-01\n",
    "# train.Date.max()  # 2015-07-31\n",
    "# len(train) # 1,017,209\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_names.columns # StateName, State\n",
    "# store_states.columns # State\n",
    "# weather.columns # file\n",
    "# google.columns # file\n",
    "\n",
    "# state_names['tuple'] = [tuple(x) for x in state_names.values]\n",
    "# state_names.tuple\n",
    "# (BadenWuerttemberg, BW)\n",
    "# (Bayern, BY)\n",
    "# (Berlin, BE)\n",
    "# (Brandenburg, BB)\n",
    "# (Bremen, HB)\n",
    "# (Hamburg, HH)\n",
    "# (Hessen, HE)\n",
    "# (MecklenburgVorpommern, MV)\n",
    "# (Niedersachsen, HB,NI)\n",
    "# (NordrheinWestfalen, NW)\n",
    "# (RheinlandPfalz, RP)\n",
    "# (Saarland, SL)\n",
    "# (Sachsen, SN)\n",
    "# (SachsenAnhalt, ST)\n",
    "# (SchleswigHolstein, SH)\n",
    "# (Thueringen, TH)\n",
    "\n",
    "#store_states.State.nunique() # 12\n",
    "#sorted(store_states.State.unique()) # 12\n",
    "\n",
    "#store_states.State.nunique() # 12\n",
    "#sorted(store_states.State.unique()) # 12\n",
    "# ['BE', 'BW', 'BY', 'HB,NI', 'HE', 'HH', 'NW', 'RP', 'SH', 'SN', 'ST', 'TH']\n",
    "# So store states don't include Saarland (SL) and 'HB,NI' should be considered 'NI' for google purposes\n",
    "# store_states has 1115 rows, one for each store, and 12 states\n",
    "\n",
    "weather.file.nunique() # 16\n",
    "sorted(weather.file.unique()) # 16\n",
    "# ['BadenWuerttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hessen', 'MecklenburgVorpommern',\n",
    "#  'Niedersachsen', 'NordrheinWestfalen', 'RheinlandPfalz', 'Saarland', 'Sachsen', 'SachsenAnhalt',\n",
    "#  'SchleswigHolstein', 'Thueringen']\n",
    "\n",
    "len(google.file.unique()) # 14\n",
    "sorted(google.file.unique()) # Take the last two and get\n",
    "# ['BE', 'BW', 'BY', 'DE', 'HE', 'HH', 'NI', 'NW', 'RP', 'SH', 'SL', 'SN', 'ST', 'TH']\n",
    "# Note 'DE' - is it a catchall for the others?  Or should others be considered a NaN?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
