{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got  google googletrend.csv\n",
      "Got  state_names state_names.csv\n",
      "Got  stores store.csv\n",
      "Got  store_states store_states.csv\n",
      "Got  train train.csv\n",
      "Got  weather weather.csv\n"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "names = ['google', 'state_names', 'stores', 'store_states', 'train', 'weather']  \n",
    "\n",
    "files = ['googletrend.csv', 'state_names.csv', 'store.csv', 'store_states.csv', 'train.csv', 'weather.csv']  \n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for name, file in zip(names, files):\n",
    "    dfs[name] =  pd.read_csv(f'../../data/raw/{file}', low_memory=False)\n",
    "    print(\"Got \", name, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = dfs['google'].copy()\n",
    "state_names = dfs['state_names'].copy()  # Done\n",
    "stores = dfs['stores'].copy()\n",
    "store_states = dfs['store_states'].copy()  # Done\n",
    "train = dfs['train'].copy()  # Done\n",
    "weather = dfs['weather'].copy()\n",
    "all_dfs = [google, state_names, stores, store_states, train, weather]\n",
    "\n",
    "# Create _raw copies for reference\n",
    "google_raw = dfs['google']\n",
    "state_names_raw = dfs['state_names']\n",
    "stores_raw = dfs['stores']\n",
    "store_states_raw = dfs['store_states']\n",
    "train_raw = dfs['train']\n",
    "weather_raw = dfs['weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spelling error in weather dataframe\n",
    "if 'Min_VisibilitykM' in weather.columns:\n",
    "    weather.rename(columns = {'Min_VisibilitykM':'Min_VisibilityKm'}, inplace=True)\n",
    "if 'Min_DewpointC' in weather.columns:                                                                                                                                    \n",
    "    weather.rename(columns={'Min_DewpointC': 'MinDew_pointC'}, inplace=True) \n",
    "\n",
    "def convert_to_snake_case(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    draft = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    return draft.replace('__', '_')\n",
    "\n",
    "for df in all_dfs:\n",
    "    col_list = list(df.columns)\n",
    "    df.columns = pd.Index(map(convert_to_snake_case, col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info()   - Stores - CompetitionOpenSinceMonth fill with mean                                      \n",
    "#   - Stores - CompetitionOpenSinceYear fill with mean                                       \n",
    "#   - Stores - Promo2SinceWeek fill with zero                                                \n",
    "#   - Stores - Promo2SinceYear fill with zero                                                \n",
    "#   - Stores - PromoInterval fill with zero                                                  \n",
    "#   - Weather - Max_VisibityKm fill with mean                                                \n",
    "#   - Weather - Min_VisibitykM fill with mean                                                \n",
    "#   - Weather - Mean_VisibityKm fill with mean                                               \n",
    "#   - Weather - Max_Gust_SpeedKm_h fill with mean                                            \n",
    "#   - Weather - CloudCover fill with mean                                                    \n",
    "#   - Weather - CloudCover fill with string 'No events'    \n",
    "\n",
    "stores['promo2_since_week'] = stores.promo2_since_week.fillna(stores.promo2_since_week.mean())\n",
    "stores['promo2_since_year'] = stores.promo2_since_year.fillna(stores.promo2_since_year.mean())\n",
    "stores['promo_interval'] = stores.promo_interval.fillna('None')\n",
    "stores['competition_distance'] = stores.competition_distance.fillna(stores.competition_distance.mean())\n",
    "stores['competition_open_since_month'] = stores.competition_open_since_month.fillna(stores.competition_open_since_month.mean())\n",
    "stores['competition_open_since_year'] = stores.competition_open_since_year.fillna(stores.competition_open_since_year.mean())\n",
    "\n",
    "weather['max_visibility_km'] = weather.max_visibility_km.fillna(weather.max_visibility_km.mean())\n",
    "weather['min_visibility_km'] = weather.min_visibility_km.fillna(weather.min_visibility_km.mean())\n",
    "weather['mean_visibility_km'] = weather.mean_visibility_km.fillna(weather.mean_visibility_km.mean())\n",
    "weather['max_gust_speed_km_h'] = weather.max_gust_speed_km_h.fillna(weather.max_gust_speed_km_h.mean())\n",
    "weather['cloud_cover'] = weather.cloud_cover.fillna(weather.cloud_cover.mean())\n",
    "weather['events'] = weather.events.fillna('No Events')\n",
    "\n",
    "# ADD OTHER CHECKS LIKE THIS\n",
    "# stores_raw.Promo2SinceWeek.mean() == stores.Promo2SinceWeek.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep\n",
    "google['state'] = google.file.str[-2:]\n",
    "google.loc[google.state == 'NI', 'state'] = 'HB,NI'\n",
    "google['week_start'] = pd.to_datetime(google.week.str[:10])\n",
    "\n",
    "# Add date to google dataframe, aligned with starting week\n",
    "start_date = pd.to_datetime(google.week.min()[:10])\n",
    "end_date = pd.to_datetime(google.week.max()[-10:])\n",
    "\n",
    "# create a new dataframe, week_lookup, listing all days in the                                                                                                                        \n",
    "# period and their corresponding week                                                                                                                                                 \n",
    "days = pd.date_range(start_date, end_date, freq='D')                                                                                                                                  \n",
    "week_lookup = pd.DataFrame({'date': days})                                                                                                                                            \n",
    "week_lookup['num'] = week_lookup['date'].dt.dayofweek                                                                                                                                 \n",
    "week_lookup['offset'] = (week_lookup['num'] + 1) % 7                                                                                                                                  \n",
    "week_lookup['Week_Start'] = week_lookup['date'] - pd.to_timedelta(week_lookup['offset'], unit='D')                                                                                                                                  \n",
    "week_lookup.drop(['num', 'offset'], axis='columns', inplace=True)\n",
    "\n",
    "#days = np.arange(start_date, end_date + pd.to_timedelta('1D'), pd.to_timedelta('1D'))\n",
    "#weeks = np.arange(start_date, end_date + pd.to_timedelta('1D'), pd.to_timedelta('7D'))\n",
    "#all_weeks = pd.Series(np.hstack([weeks for i in range(0,7)])) # 1036\n",
    "#week_lookup = pd.DataFrame({'date': days, 'Week_Start': all_weeks})\n",
    "google = week_lookup.merge(google, left_on='Week_Start', right_on='week_start')\n",
    "google.drop(['file', 'Week_Start', 'week'], axis='columns', inplace=True) #len=14504\n",
    "google = google[(google.date >= pd.to_datetime('2013-01-01')) & (google.date <= pd.to_datetime('2015-07-31'))]\n",
    "\n",
    "# Merge order after changing files:\n",
    "# store_states and state_names, on='State'\n",
    "# that and weather, left_on='StateName', right_on='file'\n",
    "# that and google, on=['Date', 'State'], ??how='outer'??\n",
    "# that and stores, on='Store'\n",
    "# that and train, on=['Date', 'Store'], ??how='outer'??\n",
    "# all_dfs = [google, state_names, stores, store_states, train, weather]\n",
    "# [, train]\n",
    "\n",
    "# Keep and refactor\n",
    "df = store_states.merge(state_names, on='state')\n",
    "# After merging, drop both file and state_name - they are colinear with 'state'\n",
    "df = df.merge(weather, left_on='state_name', right_on='file').drop(['file', 'state_name'], axis='columns')\n",
    "df = df.merge(stores, on='store')\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.merge(train, on=['date', 'store'], how='outer')\n",
    "# train.csv only goes through 2015-07-31\n",
    "# See if doing this on google instead fixes this\n",
    "# df = df[df.date <= pd.to_datetime('2015-07-31')]\n",
    "df = df.merge(google, on=['date', 'state'])\n",
    "df.loc[df.open.isnull(), 'open'] = 0\n",
    "df.loc[df.sales.isnull(), 'sales'] = 0\n",
    "df.loc[df.customers.isnull(), 'customers'] = 0\n",
    "df.loc[df.promo.isnull(), 'promo'] = 0\n",
    "df.loc[df.school_holiday.isnull(), 'school_holiday'] = 0\n",
    "df.loc[df.state_holiday.isnull(), 'state_holiday'] = '0'\n",
    "df['day_of_week'] = df.date.dt.dayofweek\n",
    "df.loc[df.customers == 0, 'open'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/processed/wrangled_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)\n",
    "# df.date.nunique() * df.store.nunique() == len(df) # True\n",
    "# df.isnull().any().any() # False\n",
    "# df.info()\n",
    "# df.state_holiday.unique()\n",
    "# len(train[(train.sales == 0) & (train.open == 1)]) # 54\n",
    "\n",
    "# DIG BACK INTO THIS -- THIS IS OK.  THERE ARE 33121 INSTANCES IN TRAIN.CSV WITHIN THE DATE RANGE\n",
    "# WHERE A STORE DOES NOT HAVE A ROW OF DATA. IN THOSE INSTANCES, WE ASSIGNED OPEN = 0 AND SALES = 0.\n",
    "# IN OTHER WORDS, WE'RE ASSUMING THAT WITHIN THE DATE RANGE, IF WE DIDN'T HAVE A ROW OF DATA FOR \n",
    "# THAT STORE, IT MEANS THAT THE STORE WAS CLOSED.\n",
    "# len(train[(train.sales == 0) & (train.open == 0)]) # 172817\n",
    "# len(df[(df.sales == 0) & (df.open == 0)]) # 205938\n",
    "# 205938-172817 # 33121\n",
    "# len(train) #1017209\n",
    "# len(df) # 1050330\n",
    "# 1050330 - 1017209 # 33121\n",
    "# train.date.min() # Timestamp('2013-01-01 00:00:00')\n",
    "# train.date.max() # Timestamp('2015-07-31 00:00:00')\n",
    "# df.date.min() # Timestamp('2013-01-01 00:00:00')\n",
    "# df.date.max() # Timestamp('2015-07-31 00:00:00')\n",
    "# df.date.max() - df.date.min() # 941 days\n",
    "# train.store.nunique() # 1115\n",
    "# train.date.nunique() # 942\n",
    "# 33121/1115 # 29.70493273542601\n",
    "# 33121/942 # 35.160297239915074\n",
    "# 1115 * 942 # 1050330\n",
    "\n",
    "# DIG BACK INTO THIS -- THIS IS OK. THE LONGEST DISTANCE IS 46 MILES, AND ONLY TWO STORES HAVE A\n",
    "# DISTANCE OF GREATER THAN 30 MILES. SEEMS HARD TO BELIEVE THAT A DRUGSTORE IN GERMANY WOULDN'T\n",
    "# HAVE ANY COMPETITION WITHIN 45 MILES, BUT MAYBE THEY'RE USING A TIGHT DEFINITION OF COMPETITOR.\n",
    "# IN ANY CASE, WE DON'T HAVE COMPELLING EVIDENCE THAT IT'S WRONG, PLUS IT'S NOT A LARGE ENOUGH\n",
    "# GROUP NOR A LARGE ENOUGH OUTLIER TO DISTORT THE RESULTS.\n",
    "# df.competition_distance.max() # 75860 -- this is 46 miles\n",
    "len(df[df.competition_distance == 75860])\n",
    "len(stores[stores.competition_distance == 75860]) # 1, store=453\n",
    "len(stores[stores.competition_distance > 10000]) # 187 -- this is 6 miles\n",
    "len(df[df.competition_distance > 10000])/942 # 187 -- this is 6 miles\n",
    "# stores.competition_distance.isnull().any() # False\n",
    "len(df[df.competition_distance > 50000])/942 # 187 -- this is 6 miles\n",
    "\n",
    "# DIG BACK INTO THIS -- THERE ARE 54 STORES THAT ARE MARKED AS OPEN BUT HAVE NO SALES. 52 OF THEM\n",
    "# ARE ALSO MARKED AS HAVING NO CUSTOMERS. THE TWO THAT HAVE CUSTOMERS BUT NO SALES HAVE 3 AND 5\n",
    "# CUSTOMERS RESPECTIVELY. WHILE IT'S POSSIBLE THAT A STORE COULD BE OPEN BUT HAVE NO CUSTOMERS\n",
    "# AND NO SALES, IT SEEMS MORE LIKELY TO BE A MISTAKE.  IN FACT, THERE ARE ONLY 6 OTHER INSTANCES\n",
    "# OF A STORE BEING OPEN AND GETTING LESS THAN 50 CUSTOMERS (COUNTING THE TWO ALREADY MENTIONED).\n",
    "# SO WE'LL MARK ANY STORE WITH 0 CUSTOMERS AS NOT BEING OPEN ON THAT DAY.\n",
    "# len(df[(df.sales == 0) & (df.open == 1)]) # 54\n",
    "# len(train[(train.sales == 0) & (train.open == 1)]) # 54\n",
    "# len(df[(df.sales == 0) & (df.open == 1) & (df.customers != 0)]) # 2\n",
    "# len(df[(df.open == 1) & (df.customers < 50)]) # 60\n",
    "# len(df[(df.open == 1) & (df.customers == 0)]) # 52\n",
    "# len(df[(df.open == 1) & (df.customers < 100)]) # 123\n",
    "# len(df[(df.sales == 0) & (df.open == 1)]) # 54 originally -- 2 after the fix\n",
    "\n",
    "# TEST THIS - OK, CHECKS OUT \n",
    "# (df.day_of_week == df.date.dt.dayofweek).all() # TRUE\n",
    "# len(pd.DataFrame(df.groupby(['store', 'date']).size().rename('Freq'))) # 1050330\n",
    "# df.groupby(['store', 'date']).ngroups #1050330\n",
    "# len(pd.unique(df[['store', 'date']].values.ravel('K'))) # 2057\n",
    "# 1115 + 942 # 2057\n",
    "# len(pd.unique(df[['store', 'date']].values.ravel('K'))) # 2057\n",
    "\n",
    "# CHECK THAT EACH (STORE, DATE) TUPLE IS UNIQUE\n",
    "(df.groupby(['store', 'date']).size() == 1).all() # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assortment',\n",
       " 'competition_distance',\n",
       " 'competition_open_since_month',\n",
       " 'competition_open_since_year',\n",
       " 'promo2',\n",
       " 'promo2_since_week',\n",
       " 'promo2_since_year',\n",
       " 'promo_interval',\n",
       " 'store',\n",
       " 'store_type']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google_raw.file.nunique() # 14\n",
    "# google_raw.week.nunique() # 148\n",
    "# len(google_raw) # 2072\n",
    "# 148 * 14 # 2072\n",
    "store_states.columns # 'store', 'state'\n",
    "state_names.columns # 'state_name', 'state'\n",
    "state_names_raw.columns # 'state_name', 'state'\n",
    "# sorted(state_names.columns) == ['state', 'state_name'] \n",
    "# a = sorted(store_states.columns) # ['state', 'store']\n",
    "# a = sorted(state_names.columns) # ['state', 'state_name']\n",
    "a = sorted(weather.columns) # ['cloud_cover', 'date', 'dew_point_c', 'events', 'file', 'max_gust_speed_km_h',\n",
    " # 'max_humidity', 'max_sea_level_pressureh_pa', 'max_temperature_c', 'max_visibility_km', 'max_wind_speed_km_h',\n",
    " # 'mean_dew_point_c', 'mean_humidity', 'mean_sea_level_pressureh_pa', 'mean_temperature_c', 'mean_visibility_km',\n",
    " # 'mean_wind_speed_km_h', 'min_dewpoint_c', 'min_humidity', 'min_sea_level_pressureh_pa', 'min_temperature_c',\n",
    " # 'min_visibility_km', 'precipitationmm', 'state', 'state_name', 'store', 'wind_dir_degrees']\n",
    "sorted(train.columns) # ['customers', 'date', 'day_of_week', 'open', 'promo', 'sales', 'school_holiday',\n",
    " # 'state_holiday', 'store']\n",
    "sorted(stores.columns) # ['assortment', 'competition_distance', 'competition_open_since_month',\n",
    " # 'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'promo_interval', 'store',\n",
    " # 'store_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names = ['google', 'state_names', 'stores', 'store_states', 'train', 'weather']  \n",
    "\n",
    "state_names_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.DataFrame({'file': ['DE_BB', 'DE_BE', np.nan], 'week': ['2013-01-01 - 2013-01-07', '2013-01-15 - 2013-01-21', 'dropped'], 'trend': [98, 69, 12]})  \n",
    "fake.dropna(axis='index', inplace=True) \n",
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.DataFrame({'file': ['DE_BB', 'DE_BE'], 'week': ['2013-01-06 - 2013-01-12', '2013-01-20 - 2013-01-26'], 'trend': [98, 69]})  \n",
    "fake['state'] = fake.file.str[-2:]\n",
    "fake.loc[fake.state == 'NI', 'state'] = 'HB,NI'\n",
    "fake['week_start'] = pd.to_datetime(fake.week.str[:10])\n",
    "start_date = pd.to_datetime(fake.week.min()[:10])\n",
    "end_date = pd.to_datetime(fake.week.max()[-10:])\n",
    "#days = np.arange(start_date, end_date + pd.to_timedelta('1D'), pd.to_timedelta('1D'))\n",
    "days = pd.date_range(start_date, end_date, freq='D')\n",
    "#weeks = np.arange(start_date, end_date + pd.to_timedelta('1D'), pd.to_timedelta('7D'))\n",
    "#all_weeks = pd.Series(sorted(np.hstack([weeks for i in range(0,7)]))) # 1036\n",
    "#week_lookup = pd.DataFrame({'date': days, 'Week_Start': all_weeks})\n",
    "#fake = week_lookup.merge(fake, left_on='Week_Start', right_on='week_start')\n",
    "#fake = fake.drop(['file', 'Week_Start', 'week'], axis='columns') #len=14504\n",
    "#fake\n",
    "#fake.Week_Start.dtype # dtype('<M8[ns]')\n",
    "#fake.week_start.dtype # dtype('<M8[ns]')\n",
    "a = pd.DataFrame({'date': days})\n",
    "a['num'] = a['date'].dt.dayofweek\n",
    "a['off2'] = (a['num'] + 1) % 7\n",
    "a['Week_Start'] = a['date'] - pd.to_timedelta(a['off2'], unit='D')\n",
    "a.drop(['num', 'off2'], axis='columns', inplace=True)\n",
    "#a['week'] = a.loc[a.num != 6, a.day + pd.to_timedelta(f'{(a.num + 1) % 6}D')]\n",
    "fake.merge(a, left_on='week_start', right_on='Week_Start')\n",
    "# a['week'] = a['day'] - pd.offsets.Week(-1)\n",
    "#a['week'] = a['day'] - pd.offsets.Week(weekday=6)\n",
    "#a['week'] = a['day'] - pd.offsets.DateOffset(weekday=6)\n",
    "#week_of_offsets = pd.to_timedelta([f'{i}D' for i in range(7)]).T\n",
    "#num_of_weeks = len(a) // 7\n",
    "#offsets = pd.Series(week_of_offsets for week in range(num_of_weeks))\n",
    "#offsets.reshape(1, -1)\n",
    "#start_string = str(start_date)[:10]\n",
    "#end_string = str(end_date)[:10]\n",
    "#range = pd.timedelta_range(start=start_string, end=end_string, freq='D')\n",
    "#range\n",
    "#pd.TimedeltaIndex.to_series(range)\n",
    "#a['week'] = a['day'] - offsets\n",
    "#a\n",
    "#(pd.to_timedelta([f'{i}D' for i in range(6)])) + (pd.to_timedelta([f'{i}D' for i in range(6)]))\n",
    "#pd.to_timedelta('1D')\n",
    "# [f'{i}D' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()\n",
    "df.info()\n",
    "train.info()\n",
    "#len(df)    # 1050330\n",
    "#942 * 1115 # 1050330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{word}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google ['file', 'trend', 'week']\n",
      "state_names ['State', 'StateName']\n",
      "stores ['Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Store', 'StoreType']\n",
      "store_states ['State', 'Store']\n",
      "train ['Customers', 'Date', 'DayOfWeek', 'Open', 'Promo', 'Sales', 'SchoolHoliday', 'StateHoliday', 'Store']\n",
      "weather ['CloudCover', 'Date', 'Dew_PointC', 'Events', 'Max_Gust_SpeedKm_h', 'Max_Humidity', 'Max_Sea_Level_PressurehPa', 'Max_TemperatureC', 'Max_VisibilityKm', 'Max_Wind_SpeedKm_h', 'MeanDew_PointC', 'Mean_Humidity', 'Mean_Sea_Level_PressurehPa', 'Mean_TemperatureC', 'Mean_VisibilityKm', 'Mean_Wind_SpeedKm_h', 'Min_DewpointC', 'Min_Humidity', 'Min_Sea_Level_PressurehPa', 'Min_TemperatureC', 'Min_VisibilitykM', 'Precipitationmm', 'WindDirDegrees', 'file']\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    print(name, sorted(dfs[name].columns))\n",
    "\n",
    "# date: google, train, weather\n",
    "# state: google, state_names, store_states, weather\n",
    "# store: store, store_states, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train) # 1017209\n",
    "# train.Date.nunique() # 942\n",
    "# train.Store.nunique() # 1115\n",
    "1115 * 942 # 1050330 \n",
    "1050330 - 1017209 # 33121\n",
    "33121/942 # 35.16...\n",
    "33121/1115 # 29.70...\n",
    "# train.groupby('Store')['Date'].count().value_counts()\n",
    "#    942    934\n",
    "#    758    180\n",
    "#    941      1\n",
    "# 934 * 942 + 180 * 758 + 941 # 1017209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.promo.unique()\n",
    "len(train[train.promo == 0]) #629129\n",
    "len(train[train.promo == 1]) #388080\n",
    "len(df.loc[df.school_holiday.isnull(), 'date'].unique()) # 185\n",
    "len(df.loc[df.state_holiday.isnull(), 'date'].unique()) # 185\n",
    "dates = np.sort(df.loc[df.state_holiday.isnull(), 'date'].unique())\n",
    "#state_names_raw.columns\n",
    "#df.state_holiday.unique()\n",
    "#df.loc[df.state_holiday.isnull(), ['date', 'state', 'state_holiday']].head()\n",
    "dates[:15]\n",
    "#type(dates)\n",
    "#df.loc[df.date.apply(lambda x: x in dates), ['date', 'state', 'state_holiday']].head()\n",
    "df.loc[df.date == '2014-07-04T00:00:00.000000000', ['date', 'state', 'state_holiday']].head()\n",
    "len(df[df.state_holiday.isnull()])\n",
    "#df['date_state'] = df.date.dt.strftime('%Y-%m-%d') + df.state\n",
    "df.date_state.tail()\n",
    "df.loc[df.state_holiday.isnull(), 'date_state'].nunique() # 185\n",
    "date_states = np.sort(df.loc[df.state_holiday.isnull(), 'date_state'].unique())\n",
    "#df[df.date_state]\n",
    "date_states[:15]\n",
    "#df.loc[(df.date=='2014-07-03') & (df.state == 'BY') & (df.state_holiday.notnull()), 'state_holiday']\n",
    "df.head()\n",
    "df = df.set_index('date') #44\n",
    "date_mask = dates\n",
    "df[date_mask].head()\n",
    "#df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "# df.isnull().sum()\n",
    "sorted(df.day_of_week.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.info()\n",
    "#df.info()\n",
    "train.head()\n",
    "len(train) # 1017209\n",
    "len(df) # 1050330\n",
    "#train['date'] = pd.to_datetime(train['date'])\n",
    "#df = df.merge(train, on=['date', 'store'])\n",
    "len(df) # 1017209\n",
    "train.date.nunique() # 942\n",
    "train.store.nunique() # 1115\n",
    "1115 * 942 # 1050330\n",
    "df.date.nunique() # 942\n",
    "df.store.nunique() # 1115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester = train.merge(stores.merge(store_states.merge(state_names, on='State'), on='Store'), on='Store')\n",
    "# Still left: google, weather\n",
    "# len(train.columns) # 9\n",
    "#tester = train.merge(tester)\n",
    "#len(train.Store)\n",
    "#train.Store.nunique()\n",
    "# tester\n",
    "len(sorted(weather.file.unique())) # 16\n",
    "#len(sorted(tester.StateName.unique())) #12\n",
    "# tester.head()\n",
    "#print(sorted(tester.StateName.unique())) #12 missing Brandenburg, Bremen, MecklenburgVorpommern, Saarland\n",
    "(sorted(weather.file.unique())) # 16\n",
    "#sorted(weather.columns)#()\n",
    "# len(tester.State.unique()) #12\n",
    "#len(tester.StateName.unique()) #12\n",
    "\n",
    "\n",
    "# This is the command that led to a runaway process\n",
    "# tester.merge(weather, left_on='StateName', right_on='file').head()\n",
    "\n",
    "# stores2, store_states, state_names, weather2 - only left google and train\n",
    "# tester = (google.merge(stores2.merge(store_states.merge(state_names, on='State'), on='Store')).merge(weather2, left_on='StateName', right_on='file')\n",
    "tester.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(google.head())\n",
    "len(google.file.unique()) # 14\n",
    "len(tester.State.unique()) # 12\n",
    "len(tester.StateName.unique()) # 12\n",
    "len(tester.file.unique()) # 12\n",
    "sorted(weather.file.unique())\n",
    "# ['BadenWuerttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hessen', 'MecklenburgVorpommern', 'Niedersachsen', 'NordrheinWestfalen', 'RheinlandPfalz', 'Saarland', 'Sachsen', 'SachsenAnhalt', 'SchleswigHolstein', 'Thueringen']\n",
    "sorted(google.file.unique())\n",
    "# ['Rossmann_DE', 'Rossmann_DE_BE', 'Rossmann_DE_BW', 'Rossmann_DE_BY', 'Rossmann_DE_HE', 'Rossmann_DE_HH', 'Rossmann_DE_NI', 'Rossmann_DE_NW', 'Rossmann_DE_RP', 'Rossmann_DE_SH', 'Rossmann_DE_SL', 'Rossmann_DE_SN', 'Rossmann_DE_ST', 'Rossmann_DE_TH']\n",
    "state_names.head()\n",
    "len(state_names.State.unique()) # 16\n",
    "(state_names.State.unique()) # 16\n",
    "# 'BW', 'BY', 'BE', 'BB', 'HB', 'HH', 'HE', 'MV', 'HB,NI', 'NW', 'RP', 'SL', 'SN', 'ST', 'SH', 'TH']\n",
    "# len(state_names.StateName.unique()) # 16\n",
    "(state_names.StateName.unique()) # 16\n",
    "# ['BadenWuerttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hessen', 'MecklenburgVorpommern', 'Niedersachsen', 'NordrheinWestfalen', 'RheinlandPfalz', 'Saarland', 'Sachsen', 'SachsenAnhalt', 'SchleswigHolstein', 'Thueringen'],\n",
    "state_names\n",
    "google2 = google.copy()\n",
    "google2['State'] = google2.file.str[-2:]\n",
    "len(google2.State.unique()) # 14\n",
    "len(google.file.unique()) #14\n",
    "\n",
    "sorted(google2.State.unique()) # 14\n",
    "# ['BE', 'BW', 'BY', 'DE', 'HE', 'HH', 'NI', 'NW', 'RP', 'SH', 'SL', 'SN', 'ST', 'TH']\n",
    "#sorted(google.file.unique()) # 14\n",
    "#['Rossmann_DE',\n",
    "# 'Rossmann_DE_BE',\n",
    "# 'Rossmann_DE_BW',\n",
    "# 'Rossmann_DE_BY',\n",
    "# 'Rossmann_DE_HE',\n",
    "# 'Rossmann_DE_HH',\n",
    "# 'Rossmann_DE_NI',\n",
    "# 'Rossmann_DE_NW',\n",
    "# 'Rossmann_DE_RP',\n",
    "# 'Rossmann_DE_SH',\n",
    "# 'Rossmann_DE_SL',\n",
    "# 'Rossmann_DE_SN',\n",
    "# 'Rossmann_DE_ST',\n",
    "# 'Rossmann_DE_TH']\n",
    "sorted(state_names.StateName.unique()) # 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(stores.Store.values) == list(range(1,1116)) # True\n",
    "# stores.CompetitionOpenSinceMonth.hist()\n",
    "#stores[stores.CompetitionOpenSinceYear>2000].CompetitionOpenSinceYear.hist()\n",
    "stores.CompetitionOpenSinceYear.mean() # 2008\n",
    "#stores[stores.CompetitionOpenSinceYear>1980].CompetitionOpenSinceYear.mean() # 2008\n",
    "stores[stores.CompetitionOpenSinceYear>2000].CompetitionOpenSinceYear.mean() # 2009\n",
    "stores.CompetitionOpenSinceMonth.mean() # 7\n",
    "stores.describe()\n",
    "#stores.Promo2.unique() # Binary, roughly half of stores have Promo2\n",
    "#train.Store.nunique() #1115\n",
    "len(stores[stores.Promo2 == 0]) #544\n",
    "len(stores[stores.Promo2SinceWeek.isnull()]) #544\n",
    "len(stores[stores.Promo2SinceYear.isnull()]) #544\n",
    "stores[stores.Promo2SinceWeek.notnull()].head(15)\n",
    "stores.PromoInterval.unique() # Either starts in Jan, starts in Feb, or starts in Mar, with quarterly restarts\n",
    "#stores.info()\n",
    "#stores.Promo2SinceWeek.hist()\n",
    "#weather.info() # look out for Max_vis, Mean_vis, Min_vis, max_gust, cloudcover, events\n",
    "weather.Events.describe()\n",
    "#weather.Max_VisibilityKm.mean() # 24.0576\n",
    "#weather.Min_VisibilitykM.mean()  # 7.0252\n",
    "#weather.Mean_VisibilityKm.mean() # 12.2398\n",
    "#24.0576-7.0252 # 17.032\n",
    "#(weather.Max_VisibilityKm >= weather.Min_VisibilitykM).all()\n",
    "#weather[(weather.Max_VisibilityKm.notnull()) & (weather.Min_VisibilitykM.notnull()) & (weather.Max_VisibilityKm < weather.Min_VisibilitykM)] # 15,459\n",
    "#len(weather) # 15,840\n",
    "weather.Events.value_counts()\n",
    "len(weather[weather.Events.isnull()]) #3591\n",
    "len(weather) #15840\n",
    "weather.Events.describe() #11889\n",
    "11889+3591\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google.info()\n",
    "google.describe()\n",
    "# google.week.min() # 2012-12-02 - 2012-12-08\n",
    "# google.week.max() # 2015-09-27 - 2015-10-03\n",
    "# google.file.nunique() # 14\n",
    "# google.week.nunique() # 148 = total file size / 14\n",
    "sorted(google.file.unique())\n",
    "\n",
    "# train.Date.min()  # 2013-01-01\n",
    "# train.Date.max()  # 2015-07-31\n",
    "# len(train) # 1,017,209\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_names.columns # StateName, State\n",
    "# store_states.columns # State\n",
    "# weather.columns # file\n",
    "# google.columns # file\n",
    "\n",
    "# state_names['tuple'] = [tuple(x) for x in state_names.values]\n",
    "# state_names.tuple\n",
    "# (BadenWuerttemberg, BW)\n",
    "# (Bayern, BY)\n",
    "# (Berlin, BE)\n",
    "# (Brandenburg, BB)\n",
    "# (Bremen, HB)\n",
    "# (Hamburg, HH)\n",
    "# (Hessen, HE)\n",
    "# (MecklenburgVorpommern, MV)\n",
    "# (Niedersachsen, HB,NI)\n",
    "# (NordrheinWestfalen, NW)\n",
    "# (RheinlandPfalz, RP)\n",
    "# (Saarland, SL)\n",
    "# (Sachsen, SN)\n",
    "# (SachsenAnhalt, ST)\n",
    "# (SchleswigHolstein, SH)\n",
    "# (Thueringen, TH)\n",
    "\n",
    "#store_states.State.nunique() # 12\n",
    "#sorted(store_states.State.unique()) # 12\n",
    "\n",
    "#store_states.State.nunique() # 12\n",
    "#sorted(store_states.State.unique()) # 12\n",
    "# ['BE', 'BW', 'BY', 'HB,NI', 'HE', 'HH', 'NW', 'RP', 'SH', 'SN', 'ST', 'TH']\n",
    "# So store states don't include Saarland (SL) and 'HB,NI' should be considered 'NI' for google purposes\n",
    "# store_states has 1115 rows, one for each store, and 12 states\n",
    "\n",
    "weather.file.nunique() # 16\n",
    "sorted(weather.file.unique()) # 16\n",
    "# ['BadenWuerttemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hessen', 'MecklenburgVorpommern',\n",
    "#  'Niedersachsen', 'NordrheinWestfalen', 'RheinlandPfalz', 'Saarland', 'Sachsen', 'SachsenAnhalt',\n",
    "#  'SchleswigHolstein', 'Thueringen']\n",
    "\n",
    "len(google.file.unique()) # 14\n",
    "sorted(google.file.unique()) # Take the last two and get\n",
    "# ['BE', 'BW', 'BY', 'DE', 'HE', 'HH', 'NI', 'NW', 'RP', 'SH', 'SL', 'SN', 'ST', 'TH']\n",
    "# Note 'DE' - is it a catchall for the others?  Or should others be considered a NaN?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
